{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adv_statistics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still working...\n",
    "#different statistics over low level features\n",
    "\n",
    "def negentropy(A):\n",
    "    ne = []\n",
    "    for u in A:\n",
    "        a1 = 2\n",
    "        G1 = 1/a1*np.log(np.cosh(2*u))\n",
    "        v = np.random.normal(0, 1, len(u))\n",
    "        G2 = -np.exp(-np.power(v,2)/2)\n",
    "        ne.append(np.power(np.mean(G1)-np.mean(G2),2))\n",
    "    return np.array(ne)\n",
    "\n",
    "#good for discrete features\n",
    "def differences_entropy(A):\n",
    "    dH = []\n",
    "    for f in A:\n",
    "        Y = np.diff(f)\n",
    "        P = []\n",
    "        for yk in Y:\n",
    "            ntimes = Y.tolist().count(yk)\n",
    "            P.append(ntimes/len(Y))\n",
    "        print P\n",
    "        dH.append(entropy(Y)/np.log(len(Y)))\n",
    "    return np.array(dH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioBasicIO#用于读取音频文件\n",
    "import os\n",
    "import subprocess as sp\n",
    "import itertools\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    # Dataset object is composed of:\n",
    "    # data \n",
    "    # targets\n",
    "    # train and test sets for cross validation\n",
    "    # classes dictionary to map classes to numbers\n",
    "\n",
    "    def __init__(self,path,decode):\n",
    "        self.classes = {0:'W', 1:'L', 2:'E', 3:'A', 4:'F', 5:'T', 6:'N'}#一共有7种情绪，用0~6编号替代\n",
    "        self.get_berlin_dataset(path)\n",
    "\n",
    "    def get_berlin_dataset(self,path):\n",
    "        males = ['03','10','11','12','15']\n",
    "        females = ['08','09','13','14','16']\n",
    "        classes = {v: k for k, v in self.classes.iteritems()}\n",
    "        self.targets = []; self.data = []; self.train_sets = []; self.test_sets = []; get_data = True\n",
    "        for speak_test in itertools.product(males,females):#test_couples:\n",
    "            i = 0; train = []; test = [];\n",
    "            for audio in os.listdir(path):\n",
    "                audio_path = os.path.join(path,audio)\n",
    "                [Fs,x] = audioBasicIO.readAudioFile(audio_path)\n",
    "                if get_data:\n",
    "                    self.data.append((x,Fs))\n",
    "                    self.targets.append(classes[audio[5]])\n",
    "                if audio[:2] in speak_test:\n",
    "                    test.append(i)\n",
    "                else:\n",
    "                    train.append(i)\n",
    "                i = i + 1\n",
    "            self.train_sets.append(train)\n",
    "            self.test_sets.append(test)\n",
    "            get_data = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emorecgnition.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.stats import entropy\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import sleep\n",
    "import sys\n",
    "from dataset import Dataset\n",
    "from preprocessing import Eigenspectrum, Preprocessor\n",
    "#from adv_statistics import negentropy, differences_entropy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #warnings.filterwarnings('ignore')\n",
    "    #这里命令行参数解析貌似有点问题\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-d\", \"--dataset\", dest=\"db_type\", default=\"berlin\")\n",
    "    parser.add_argument(\"-p\", \"--dataset_path\", dest=\"path\", default=\"\")\n",
    "    parser.add_argument(\"-l\", \"--load_data\", action=\"store_true\", dest=\"load_data\")\n",
    "    parser.add_argument(\"-e\", \"--extract_features\", action=\"store_true\", dest=\"extract_features\")\n",
    "    parser.add_argument(\"-s\", \"--speaker_indipendence\", action=\"store_true\", dest=\"speaker_indipendence\")\n",
    "    parser.add_argument(\"-i\", \"--plot_eigenspectrum\", action=\"store_true\", dest=\"plot_eigenspectrum\")\n",
    "    (options, args) = parser.parse_args(sys.argv)\n",
    "    load_data = options.load_data\n",
    "    extract_features = options.extract_features\n",
    "    db_type = options.db_type\n",
    "    speaker_indipendence = options.speaker_indipendence\n",
    "    path = options.path\n",
    "    plot_eigenspectrum = options.plot_eigenspectrum\n",
    "\n",
    "    if load_data:\n",
    "        print(\"Loading data from \" + db_type + \" dataset...\")\n",
    "        if db_type != 'berlin':\n",
    "            sys.exit(\"Dataset not registered. Please create a method to read it\")\n",
    "\n",
    "        db = Dataset(path,db_type,decode=False)\n",
    "\n",
    "        print(\"Saving \" + db_type + \" dataset info to file...\")\n",
    "        pickle.dump(db, open(db_type + '_db.p', 'wb')) \n",
    "    else:\n",
    "        print(\"Getting data from \" + db_type + \" dataset...\")\n",
    "        db = pickle.load(open(db_type + '_db.p', 'rb'))\n",
    "\n",
    "    n_samples = len(db.targets)\n",
    "    print(\"Number of dataset samples: \" + str(n_samples))\n",
    "\n",
    "    if extract_features:\n",
    "        win_size = 0.04\n",
    "        step = 0.01\n",
    "        Fglobal = []\n",
    "        i = 0\n",
    "        for (x,Fs) in db.data:\n",
    "            F = audioFeatureExtraction.stFeatureExtraction(x, Fs, win_size*Fs, step*Fs)\n",
    "            Fglobal.append( np.concatenate((np.mean(F, axis=1),\n",
    "                                                np.std(F, axis=1))))\n",
    "\n",
    "            sys.stdout.write(\"\\033[F\") # cursor up one line\n",
    "            i = i+1; print(\"Extracting features \" + str(i) + '/' + str(n_samples) + \" from data...\")\n",
    "\n",
    "        print(\"Saving features to file...\")\n",
    "        pickle.dump(Fglobal, open(db_type + '_features.p', 'wb')) \n",
    "    else:\n",
    "        print(\"Getting features from files...\")\n",
    "        Fglobal = pickle.load(open(db_type + '_features.p', 'rb'))\n",
    "\n",
    "    Fglobal = np.array(Fglobal)\n",
    "    y = np.array(db.targets)\n",
    "\n",
    "    # evaluating SVM using cross validation\n",
    "    print(\"Evaluating model with cross validation...\")\n",
    "\n",
    "    if speaker_indipendence:\n",
    "        k_folds = len(db.test_sets)\n",
    "        splits = zip(db.train_sets,db.test_sets)\n",
    "    else:\n",
    "        k_folds = 10\n",
    "        sss = StratifiedShuffleSplit(n_splits=k_folds, test_size=0.2, random_state=1)\n",
    "        splits = sss.split(Fglobal, y)\n",
    "\n",
    "    # setting preprocessing\n",
    "    pp = Preprocessor('standard',n_components=50)\n",
    "    n_classes = len(db.classes)\n",
    "    clf = OneVsRestClassifier(svm.SVC(kernel='rbf',C=10, gamma=0.01))\n",
    "    prfs = []; scores = []; acc = np.zeros(n_classes)\n",
    "    mi_threshold = 0.0\n",
    "    for (train,test) in splits:\n",
    "        # selecting features using mutual information\n",
    "        Ftrain = Fglobal[train]; Ftest = Fglobal[test]\n",
    "        f_subset = pp.mutual_info_select(Ftrain,y[train],mi_threshold)\n",
    "        Ftrain = Ftrain[:,f_subset]; Ftest = Ftest[:,f_subset]\n",
    "\n",
    "        #standard transformation\n",
    "        (Ftrain,Ftest) = pp.standardize(Ftrain,Ftest)\n",
    "\n",
    "        # eigenspectrum over all data\n",
    "        if plot_eigenspectrum:\n",
    "            es = Eigenspectrum(Ftrain)\n",
    "            es.show()\n",
    "\n",
    "        (Ftrain,Ftest) = pp.project_on_pc(Ftrain,Ftest)\n",
    "\n",
    "        clf.fit(Ftrain, y[train])\n",
    "        ypred = clf.predict(Ftest)\n",
    "\n",
    "        #print clf.score(Ftest, y[test])\n",
    "        scores.append(clf.score(Ftest, y[test]))\n",
    "        #print precision_recall_fscore_support(y[test], ypred)\n",
    "        prfs.append(precision_recall_fscore_support(y[test], ypred))\n",
    "\n",
    "    # mean total accuracy\n",
    "    print(\"\\nAccuracy =  %0.2f (%0.2f)\\n\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "    #mean per class precision and recall \n",
    "    mean_prec = np.zeros((1,n_classes))\n",
    "    mean_recall = np.zeros((1,n_classes))\n",
    "    precs = []; recalls = []\n",
    "    for mat in prfs:\n",
    "        precs.append(mat[0])\n",
    "        recalls.append(mat[1])\n",
    "        mean_prec = mean_prec + mat[0]\n",
    "        mean_recall = mean_recall + mat[1]\n",
    "    mean_prec = mean_prec[0] / k_folds\n",
    "    mean_recall = mean_recall[0] / k_folds\n",
    "\n",
    "    #mean total recall and precision\n",
    "    precs = np.array(precs)\n",
    "    recalls = np.array(recalls)\n",
    "    prec_mean = np.mean(precs,axis=0)\n",
    "    prec_std = np.std(precs,axis=0)\n",
    "    recall_mean = np.mean(recalls,axis=0)\n",
    "    recall_std = np.std(recalls,axis=0)\n",
    "    print(\"Recall %0.2f (%0.2f)\" % (np.mean(recall_mean), np.std(recall_mean)))\n",
    "    print(\"Precision: %0.2f (%0.2f)\\n\" % (np.mean(prec_mean), np.std(prec_mean)))\n",
    "\n",
    "    for i in range(0,n_classes):\n",
    "        print(db.classes[i] + \" precision = %0.2f (%0.2f)\" % (prec_mean[i],prec_std[i]))\n",
    "        print(db.classes[i] + \" recall = %0.2f (%0.2f)\\n\" % (recall_mean[i],recall_std[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigh\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "class Eigenspectrum:\n",
    "\n",
    "    # Eigenspectrum is composed of:\n",
    "    # eigenvalues\n",
    "    # % info for each eigenvalue\n",
    "    # you can plot it using show()\n",
    "\n",
    "    def __init__(self, F):\n",
    "        self.eigenvalues = eigh(np.cov(F.T),eigvals_only=True)\n",
    "        total_inf = np.sum(self.eigenvalues)\n",
    "        self.info = self.eigenvalues/total_inf*100\n",
    "\n",
    "    def show(self):\n",
    "        info = self.info[::-1]\n",
    "        plt.plot(range(0,len(self.eigenvalues)),info)\n",
    "        plt.title('Eigenspectrum')\n",
    "        plt.show()\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    # Preprocessor is composed of:\n",
    "    # scaler (minmax or standard)\n",
    "    # a pc projector\n",
    "    # you can also remove features using mutual_info_select()\n",
    "\n",
    "    def __init__(self,scaler_type,n_components):\n",
    "        if scaler_type == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        elif scaler_type == \"minmax\":\n",
    "            self.scaler = MinMaxScaler()\n",
    "        self.pca = PCA(n_components)\n",
    "\n",
    "    def standardize(self,Ftrain,Ftest):\n",
    "        Ftrain_std = self.scaler.fit_transform(Ftrain)\n",
    "        Ftest_std = self.scaler.transform(Ftest)\n",
    "        return (Ftrain_std,Ftest_std)\n",
    "\n",
    "    def project_on_pc(self,Ftrain,Ftest):\n",
    "        Ftrain_pca = self.pca.fit_transform(Ftrain)\n",
    "        Ftest_pca = self.pca.transform(Ftest)\n",
    "        return (Ftrain_pca,Ftest_pca)\n",
    "\n",
    "    def mutual_info_select(self,F,y,threshold):\n",
    "        mi = list(enumerate(mutual_info_classif(F,y)))\n",
    "        f_best = []\n",
    "        for (ind,rank) in mi:\n",
    "            if rank > threshold:\n",
    "                f_best.append(ind)\n",
    "        return f_best\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简版\n",
    "先写一个脚本，将wav中不同类别的语音分开保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emo-DB数据集数据命名方式  \n",
    "03a01Fa.wav  \n",
    "只需要关注倒数第二个字符，它代表着情绪  \n",
    "W表示生气、L表示无聊、E表示恶心、A表示愤怒、F表示高兴、T表示伤心、N表示中性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filing(path,root):\n",
    "    import os,shutil\n",
    "    file= os.listdir(path)\n",
    "    dic= dict({'W':'angry','L':'boring','E':'disgust','A':'axiety','F':'happy','T':'sad','N':'neutral'})\n",
    "    for item in file:\n",
    "        A,B= item.split('.')\n",
    "        key= A[-2] \n",
    "        temp= os.path.join(root, dic[key])\n",
    "        if not os.path.exists(temp):\n",
    "            os.mkdir(temp)\n",
    "        else:\n",
    "            shutil.copy(os.path.join(path, item),os.path.join(temp,item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= r'C:\\Users\\Administrator\\Desktop\\EMO-DB\\wav'\n",
    "root_path= r'C:\\Users\\Administrator\\Desktop\\wav'\n",
    "filing(file_path, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "import numpy as np\n",
    "def train(wave,win_size, step_size):\n",
    "    file= os.listdir(wave)\n",
    "    data= []\n",
    "    label= []\n",
    "    for i in file:\n",
    "        each_data= []\n",
    "        temp= os.path.join(wave,i)\n",
    "        for j in os.listdir(temp):\n",
    "            [Fs, x]= audioBasicIO.readAudioFile(os.path.join(temp,j))\n",
    "            feature, feature_name=audioFeatureExtraction.stFeatureExtraction(x, Fs, win_size*Fs, step_size*Fs)\n",
    "            each_data.append(np.concatenate(np.mean(feature,axis=1),np.std(feature, axis=1)))\n",
    "        data.append(each_data)\n",
    "        label.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Fs, x]=audioBasicIO.readAudioFile(r'C:\\Users\\Administrator\\Desktop\\wav\\angry\\03a02Wb.wav')\n",
    "feature, feature_name= audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.025*Fs, 0.01*Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.76643991e-01,  1.09479445e-02,  2.94315028e+00,  2.92977119e-01,\n",
       "        2.32565686e-01,  1.28156261e+00,  1.19125002e-02,  3.25690476e-01,\n",
       "       -2.92608934e+01,  1.26744341e+00,  3.66709971e-01,  5.69478920e-03,\n",
       "       -1.75188224e-01, -2.03790474e-01,  1.94795207e-02,  4.47890953e-03,\n",
       "       -2.37139206e-01,  5.30750588e-02, -1.39822202e-01,  2.99010261e-02,\n",
       "       -1.07777340e-01,  1.08306065e-02,  2.59741814e-03,  2.88713433e-02,\n",
       "        4.22013603e-03,  1.45342057e-02,  6.24775808e-03,  6.44403007e-02,\n",
       "        3.42337667e-03,  4.85564685e-03,  9.51100981e-03,  3.65615371e-02,\n",
       "        2.66389216e-03,  3.16156411e-02,  1.67446953e-01,  1.85002298e-02,\n",
       "        4.14269131e-01,  1.34732579e-01,  3.44960686e-02,  7.83674009e-01,\n",
       "        2.45284685e-02,  2.64638791e-01,  4.66427560e+00,  1.20528791e+00,\n",
       "        1.15387722e+00,  8.62239317e-01,  4.37379073e-01,  4.30020799e-01,\n",
       "        3.74374244e-01,  3.48501112e-01,  3.91568739e-01,  3.62246546e-01,\n",
       "        3.02480243e-01,  3.55483175e-01,  3.22330585e-01,  2.49163014e-02,\n",
       "        6.20586272e-03,  3.81015575e-02,  6.54186009e-03,  2.44234450e-02,\n",
       "        1.20534299e-02,  1.09198393e-01,  6.29853117e-03,  7.52354233e-03,\n",
       "        1.39599340e-02,  5.40393566e-02,  5.06945624e-03,  2.76509253e-02])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((np.mean(feature,axis=1),np.std(feature, axis=1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.76643991e-01,  1.09479445e-02,  2.94315028e+00,  2.92977119e-01,\n",
       "        2.32565686e-01,  1.28156261e+00,  1.19125002e-02,  3.25690476e-01,\n",
       "       -2.92608934e+01,  1.26744341e+00,  3.66709971e-01,  5.69478920e-03,\n",
       "       -1.75188224e-01, -2.03790474e-01,  1.94795207e-02,  4.47890953e-03,\n",
       "       -2.37139206e-01,  5.30750588e-02, -1.39822202e-01,  2.99010261e-02,\n",
       "       -1.07777340e-01,  1.08306065e-02,  2.59741814e-03,  2.88713433e-02,\n",
       "        4.22013603e-03,  1.45342057e-02,  6.24775808e-03,  6.44403007e-02,\n",
       "        3.42337667e-03,  4.85564685e-03,  9.51100981e-03,  3.65615371e-02,\n",
       "        2.66389216e-03,  3.16156411e-02])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16744695, 0.01850023, 0.41426913, 0.13473258, 0.03449607,\n",
       "       0.78367401, 0.02452847, 0.26463879, 4.6642756 , 1.20528791,\n",
       "       1.15387722, 0.86223932, 0.43737907, 0.4300208 , 0.37437424,\n",
       "       0.34850111, 0.39156874, 0.36224655, 0.30248024, 0.35548318,\n",
       "       0.32233058, 0.0249163 , 0.00620586, 0.03810156, 0.00654186,\n",
       "       0.02442344, 0.01205343, 0.10919839, 0.00629853, 0.00752354,\n",
       "       0.01395993, 0.05403936, 0.00506946, 0.02765093])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(feature,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "import numpy as np\n",
    "import os\n",
    "def train(wave,win_size, step_size):\n",
    "    file= os.listdir(wave)\n",
    "    data= []\n",
    "    label= []\n",
    "    for i in file:\n",
    "        each_data= []\n",
    "        temp= os.path.join(wave,i)\n",
    "        for j in os.listdir(temp):\n",
    "            [Fs, x]= audioBasicIO.readAudioFile(os.path.join(temp,j))\n",
    "            feature,feature_name=audioFeatureExtraction.stFeatureExtraction(x, Fs, win_size*Fs, step_size*Fs)\n",
    "            each_data.append(np.concatenate((np.mean(feature,axis=1),np.std(feature, axis=1)),axis=0))\n",
    "            label.append(i)\n",
    "        data.append(each_data)\n",
    "    return data, label\n",
    "wave= r'C:\\Users\\Administrator\\Desktop\\wav'\n",
    "data, label=train(wave, 0.025, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'C:\\Users\\Administrator\\Desktop\\test.txt',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.76643991e-01,  1.09479445e-02,  2.94315028e+00,  2.92977119e-01,\n",
       "        2.32565686e-01,  1.28156261e+00,  1.19125002e-02,  3.25690476e-01,\n",
       "       -2.92608934e+01,  1.26744341e+00,  3.66709971e-01,  5.69478920e-03,\n",
       "       -1.75188224e-01, -2.03790474e-01,  1.94795207e-02,  4.47890953e-03,\n",
       "       -2.37139206e-01,  5.30750588e-02, -1.39822202e-01,  2.99010261e-02,\n",
       "       -1.07777340e-01,  1.08306065e-02,  2.59741814e-03,  2.88713433e-02,\n",
       "        4.22013603e-03,  1.45342057e-02,  6.24775808e-03,  6.44403007e-02,\n",
       "        3.42337667e-03,  4.85564685e-03,  9.51100981e-03,  3.65615371e-02,\n",
       "        2.66389216e-03,  3.16156411e-02,  1.67446953e-01,  1.85002298e-02,\n",
       "        4.14269131e-01,  1.34732579e-01,  3.44960686e-02,  7.83674009e-01,\n",
       "        2.45284685e-02,  2.64638791e-01,  4.66427560e+00,  1.20528791e+00,\n",
       "        1.15387722e+00,  8.62239317e-01,  4.37379073e-01,  4.30020799e-01,\n",
       "        3.74374244e-01,  3.48501112e-01,  3.91568739e-01,  3.62246546e-01,\n",
       "        3.02480243e-01,  3.55483175e-01,  3.22330585e-01,  2.49163014e-02,\n",
       "        6.20586272e-03,  3.81015575e-02,  6.54186009e-03,  2.44234450e-02,\n",
       "        1.20534299e-02,  1.09198393e-01,  6.29853117e-03,  7.52354233e-03,\n",
       "        1.39599340e-02,  5.40393566e-02,  5.06945624e-03,  2.76509253e-02])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'C:\\Users\\Administrator\\Desktop\\test.txt',data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(r'C:\\Users\\Administrator\\Desktop\\test.txt','w+')\n",
    "for i1 in range(len(data)):\n",
    "    arrs= []\n",
    "    temp= data[i1]\n",
    "    for i2 in range(len(temp)):\n",
    "        joint= temp[i2]\n",
    "        arrs.append(joint)\n",
    "        for i3 in range(len(temp[0])):\n",
    "            strNum= str(joint[i3])\n",
    "            f.write(strNum)\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= np.loadtxt(r'C:\\Users\\Administrator\\Desktop\\test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 68)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train , y_test= train_test_split(Data,label, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf= SVC(kernel='rbf',C=100,gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7232704402515723"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7506775067750677"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6839304871373837"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "svm= SVC(kernel='rbf',C=100,gamma=0.1,random_state=0)\n",
    "cross_val_score(svm, Data, label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6492346918726228"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB= GaussianNB()\n",
    "cross_val_score(NB, Data, label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6593884678539851"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn. ensemble import RandomForestClassifier\n",
    "RF= RandomForestClassifier(n_estimators=100)\n",
    "cross_val_score(RF, Data, label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7258304843304845"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR= LogisticRegression(random_state=0, solver='liblinear',multi_class='auto',C=100)\n",
    "cross_val_score(LR, Data, label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5236796169985826"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn= KNeighborsClassifier(n_neighbors=7,weights='uniform')\n",
    "cross_val_score(knn, Data, label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5406694104107898"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Ada= AdaBoostClassifier(n_estimators=100,random_state=0, algorithm='SAMME')\n",
    "cross_val_score(Ada, Data, label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6578231292517007"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBDT= GradientBoostingClassifier(n_estimators= 100, random_state=0)\n",
    "cross_val_score(GBDT, Data, label, cv=20, scoring='balanced_accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7407312925170068"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda= LinearDiscriminantAnalysis(n_components=7)\n",
    "cross_val_score(lda, Data,label, cv=20, scoring='balanced_accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6925985642709781"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb= XGBClassifier(n_estimators=200)\n",
    "cross_val_score(xgb, Data,label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 68)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用librosa提取MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "wave= r'C:\\Users\\Administrator\\Desktop\\test.wav'\n",
    "fs, audio=wav.read(wave)\n",
    "mfcc_feature= mfcc(audio, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 178)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_feature.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\anaconda3\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import numpy as np\n",
    "def MFCC(wave):\n",
    "    file= os.listdir(wave)\n",
    "    data= []\n",
    "    label= []\n",
    "    for i in file:\n",
    "        each_data= []\n",
    "        temp= os.path.join(wave,i)\n",
    "        for j in os.listdir(temp):\n",
    "            sr, y= wav.read(os.path.join(temp,j))\n",
    "            feature= mfcc(y, sr).T\n",
    "            each_data.append(np.concatenate((np.mean(feature,axis=1),np.std(feature, axis=1)),axis=0))\n",
    "            label.append(i)\n",
    "        data.append(each_data)\n",
    "    return data, label\n",
    "wave= r'C:\\Users\\Administrator\\Desktop\\wav'\n",
    "data, label=MFCC(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(r'C:\\Users\\Administrator\\Desktop\\test2.txt','w+')\n",
    "for i1 in range(len(data)):\n",
    "    arrs= []\n",
    "    temp= data[i1]\n",
    "    for i2 in range(len(temp)):\n",
    "        joint= temp[i2]\n",
    "        arrs.append(joint)\n",
    "        for i3 in range(len(temp[0])):\n",
    "            strNum= str(joint[i3])\n",
    "            f.write(strNum)\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= np.loadtxt(r'C:\\Users\\Administrator\\Desktop\\test2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6761054421768707"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda= LinearDiscriminantAnalysis(n_components=7)\n",
    "cross_val_score(lda, Data,label, cv=20, scoring='balanced_accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std= StandardScaler()\n",
    "data= std.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6761054421768707"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda= LinearDiscriminantAnalysis(n_components=7)\n",
    "cross_val_score(lda, data,label, cv=20, scoring='balanced_accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= LinearDiscriminantAnalysis(n_components=7)\n",
    "clf.fit(x_train, y_train)\n",
    "y_score= clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry    0.78049   0.88889   0.83117        36\n",
      "      axiety    0.93750   0.68182   0.78947        22\n",
      "      boring    0.66667   0.72727   0.69565        22\n",
      "     disgust    0.63636   0.70000   0.66667        10\n",
      "       happy    0.61111   0.50000   0.55000        22\n",
      "     neutral    0.81481   0.88000   0.84615        25\n",
      "         sad    0.90909   0.90909   0.90909        22\n",
      "\n",
      "   micro avg    0.77358   0.77358   0.77358       159\n",
      "   macro avg    0.76515   0.75530   0.75546       159\n",
      "weighted avg    0.77716   0.77358   0.77054       159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_score, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450929294205156"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "svm= SVC(kernel='rbf',C=10,gamma=0.01,random_state=0)\n",
    "cross_val_score(svm, data, label, cv=20, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train , y_test= train_test_split(data,label, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf= SVC(kernel='rbf',C=10,gamma=0.01,random_state=0)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.779874213836478"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score,recall_score, f1_score,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123,   0,   0,   0,   3,   0,   0],\n",
       "       [  1,  62,   0,   0,   5,   0,   0],\n",
       "       [  0,   1,  67,   0,   0,  10,   2],\n",
       "       [  1,   2,   0,  38,   0,   2,   2],\n",
       "       [ 11,   2,   0,   3,  54,   0,   0],\n",
       "       [  0,   1,   4,   0,   1,  72,   0],\n",
       "       [  0,   0,   2,   0,   0,   0,  59]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(label,y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry    0.79545   0.97222   0.87500        36\n",
      "      axiety    0.80952   0.77273   0.79070        22\n",
      "      boring    0.81250   0.59091   0.68421        22\n",
      "     disgust    0.75000   0.60000   0.66667        10\n",
      "       happy    0.64706   0.50000   0.56410        22\n",
      "     neutral    0.72414   0.84000   0.77778        25\n",
      "         sad    0.87500   0.95455   0.91304        22\n",
      "\n",
      "   micro avg    0.77987   0.77987   0.77987       159\n",
      "   macro avg    0.77338   0.74720   0.75307       159\n",
      "weighted avg    0.77616   0.77987   0.77079       159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_score, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
