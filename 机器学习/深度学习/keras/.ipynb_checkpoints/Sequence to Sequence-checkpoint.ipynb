{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 一个简单的Seq2Seq例子：将中文翻译成英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, CuDNNLSTM, Dense\n",
    "from keras import callbacks\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "batch_size=64\n",
    "epochs= 100\n",
    "latent_dim= 256 # LSTM单元数量\n",
    "num_samples= 10000#训练样本大小\n",
    "dataset=\"cmn.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据读取\n",
    "input_texts=[]\n",
    "target_texts= []\n",
    "input_characters= set()\n",
    "target_characcters= set()\n",
    "with open(dataset,\"r\",encoding=\"utf-8\") as f:\n",
    "    lines= f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\t嗨。',\n",
       " 'Hi.\\t你好。',\n",
       " 'Run.\\t你用跑的。',\n",
       " 'Wait!\\t等等！',\n",
       " 'Hello!\\t你好。',\n",
       " 'I try.\\t让我来。',\n",
       " 'I won!\\t我赢了。',\n",
       " 'Oh no!\\t不会吧。',\n",
       " 'Cheers!\\t乾杯!',\n",
       " 'Got it?\\t你懂了吗？',\n",
       " 'He ran.\\t他跑了。',\n",
       " 'Hop in.\\t跳进来。',\n",
       " 'I lost.\\t我迷失了。',\n",
       " 'I quit.\\t我退出。',\n",
       " \"I'm OK.\\t我沒事。\",\n",
       " 'Listen.\\t听着。',\n",
       " 'No way!\\t不可能！',\n",
       " 'No way!\\t没门！',\n",
       " 'Really?\\t你确定？',\n",
       " 'Try it.\\t试试吧。']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分割lines，得到训练数据\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, target_text=line.split(\"\\t\")\n",
    "    target_text= \"\\t\"+target_text+\"\\n\"#用\"\\t\"作为序列开始标志，\"\\n\"作为序列结束标志\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    #计算input_text中的tokens,英文中的tokens是字符级别\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    #计算target_text中的tokens\n",
    "    for char in target_text:\n",
    "        if char not in target_characcters:\n",
    "            target_characcters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_characters= sorted(list(input_characters))\n",
    "target_characcters= sorted(list(target_characcters))\n",
    "num_encoder_tokens= len(input_characters)\n",
    "num_decoder_tokens= len(target_characcters)\n",
    "max_encoder_seq_length= max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length= max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['三', '上', '下', '不', '与', '丐', '丑', '专', '且', '世']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_characcters[60:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你说中文是怎么进行sorted的？依据ascii码？这就得了解汉字在计算机中的编码方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数量： 10000\n",
      "输入tokens数量： 73\n",
      "目标tokens数量： 2580\n",
      "输入最大长度： 30\n",
      "输出最大长度： 22\n"
     ]
    }
   ],
   "source": [
    "print(\"样本数量：\",len(input_texts))\n",
    "print(\"输入tokens数量：\",num_encoder_tokens)\n",
    "print(\"目标tokens数量：\",num_decoder_tokens)\n",
    "print(\"输入最大长度：\",max_encoder_seq_length)\n",
    "print(\"输出最大长度：\",max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#建立字符-数字字典用于字符向量化\n",
    "input_token_index= dict([(char,i) for i,char in enumerate(input_characters)])\n",
    "target_token_index= dict([(char,i) for i,char in enumerate(target_characcters)])\n",
    "#创建数组\n",
    "encoder_input_data= np.zeros((len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype=np.float32)\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n",
    "#填充数据，对每一个字符做one-hot\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    #对编码器序列做One-hot\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]]=1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i,t,target_token_index[char]]=1.0\n",
    "        if t>0:\n",
    "            decoder_target_data[i,t-1,target_token_index[char]]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义编码器输入\n",
    "encoder_inputs= Input(shape=(None, num_encoder_tokens))\n",
    "#编码器\n",
    "encoder= CuDNNLSTM(latent_dim,return_state=True)\n",
    "#调用编码器，得到编码器输入、状态信息\n",
    "encoder_output, state_h, state_c= encoder(encoder_inputs)\n",
    "#丢弃编码器的输出，我们只需要编码器的状态\n",
    "encoder_state= [state_h,state_c]\n",
    "\n",
    "#定义解码器输入\n",
    "decoder_inputs= Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm= CuDNNLSTM(latent_dim, return_sequences=True, return_state= True)\n",
    "#将编码器输出的状态作为解码器的初始状态\n",
    "decoder_outputs, _,_= decoder_lstm(decoder_inputs,initial_state=encoder_state)\n",
    "#添加全连接层\n",
    "decoder_dense= Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs= decoder_dense(decoder_outputs)\n",
    "\n",
    "#定义整个模型\n",
    "model= Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 22s 3ms/step - loss: 1.9675 - val_loss: 2.5045\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 12s 2ms/step - loss: 1.8368 - val_loss: 2.4496\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.7354 - val_loss: 2.2843\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 12s 1ms/step - loss: 1.6451 - val_loss: 2.2005\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.5577 - val_loss: 2.1139\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.4787 - val_loss: 2.0716\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.4121 - val_loss: 2.0083\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.3489 - val_loss: 1.9546\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.2926 - val_loss: 1.9197\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.2465 - val_loss: 1.9002\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.2051 - val_loss: 1.8613\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.1647 - val_loss: 1.8370\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 12s 1ms/step - loss: 1.1310 - val_loss: 1.8181\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 12s 1ms/step - loss: 1.0964 - val_loss: 1.8278\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.0667 - val_loss: 1.7975\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.0396 - val_loss: 1.7906\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 1.0452 - val_loss: 1.7974\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9974 - val_loss: 1.7782\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9605 - val_loss: 1.7767\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9346 - val_loss: 1.7705\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.9073 - val_loss: 1.7662\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8837 - val_loss: 1.7742\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8608 - val_loss: 1.7673\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8373 - val_loss: 1.7758\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8153 - val_loss: 1.7707\n",
      "Epoch 26/100\n",
      " 128/8000 [..............................] - ETA: 9s - loss: 0.7707"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "history=model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size=batch_size,epochs= epochs,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"seq2seq.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "173px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
